<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>hci on Steve's Blog</title><link>https://echowings.github.io/tags/hci/</link><description>Recent content in hci on Steve's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 01 Jul 2021 14:06:58 +0800</lastBuildDate><atom:link href="https://echowings.github.io/tags/hci/index.xml" rel="self" type="application/rss+xml"/><item><title>How to Delete Ghost Monitor in Ceph Cluster</title><link>https://echowings.github.io/p/how-to-delete-ghost-monitor-in-ceph-cluster/</link><pubDate>Thu, 01 Jul 2021 14:06:58 +0800</pubDate><guid>https://echowings.github.io/p/how-to-delete-ghost-monitor-in-ceph-cluster/</guid><description>&lt;h1 id="how-to-delete-ghost-monitor-in-ceph-cluster">How to delete ghost monitor in ceph cluster&lt;/h1>
&lt;hr>
&lt;p>Today , after I let a new server join pve cluster. ceph mon is stopped. I want to start or delete it , I always got a failed notification. After search and try to fix it. At last I fixed it.Here is what I had do&lt;/p>
&lt;h2 id="issue-description">Issue description&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">root@xa-autotest-hci04:~# pveceph createmon
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">monitor &lt;span class="s1">&amp;#39;xa-autotest-hci04&amp;#39;&lt;/span> already exists
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">root@xa-autotest-hci04:~# pveceph mon destroy xa-autotest-hci04
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">no such monitor id &lt;span class="s1">&amp;#39;xa-autotest-hci04&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="how-to-fixed-it">How to fixed it&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#stop ceph-mon@xa-autotest-hci04 service&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">systemctl stop ceph-mon@xa-autotest-hci04
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#Disable ceph-mon@xa-autotest-hci04 service&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">systemctl disable ceph-mon@xa-autotest-hci04
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pveceph purge
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">rm -rf /var/lib/ceph/mon/ceph-xa-autotest-hci04
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pveceph createmon
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#Created symlink /etc/systemd/system/ceph-mon.target.wants/ceph-mon@xa-autotest-hci04.service -&amp;gt; /lib/systemd/system/ceph-mon@.service.&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>How to Let a New Server Join Hci System</title><link>https://echowings.github.io/p/how-to-let-a-new-server-join-hci-system/</link><pubDate>Thu, 01 Jul 2021 06:38:10 +0800</pubDate><guid>https://echowings.github.io/p/how-to-let-a-new-server-join-hci-system/</guid><description>&lt;p>##How to let a new server join the exist HCI cluster&lt;/p>
&lt;h3 id="1-insall-the-pve-on-the-server">1. Insall the pve on the server&lt;/h3>
&lt;p>&amp;ndash;skip&amp;mdash;&lt;/p>
&lt;h3 id="2-let-the-new-server-join-the-pve-cluster">2. Let the new server join the pve cluster&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#ssh login the server&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pvecm add 10.32.4.37
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="3-configure-ntp-server">3. configure NTP server&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">cat &amp;gt;&amp;gt; /etc/systemd/timesyncd.conf &lt;span class="s">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">NTP=192.168.0.2 192.168.0.3
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Restart systemd-timesyncd service to apply settings&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">systemctl restart systemd-timesyncd
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="4-install-ceph">4. Install ceph&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">pveceph install
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="5-partition-nvme-disk">5. Partition nvme disk&lt;/h3>
&lt;ul>
&lt;li>partitin 1-6 for block.wal(ceph)&lt;/li>
&lt;li>partition 7-12 for block.db(ceph)&lt;/li>
&lt;li>partition 13-18 for bcache&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mktable gpt
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 1M 10G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 10G 20G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 20G 30G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 30G 40G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 40G 50G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 50G 60G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 60G 110G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 110G 160G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 160G 210G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 210G 260G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 260G 310G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 310G 360G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 360G 433G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 433G 506G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 506G 579G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 579G 652G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 652G 725G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme1n1 mkpart primary 725G 800G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mktable gpt
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 1M 10G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 10G 20G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 20G 30G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 30G 40G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 40G 50G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 50G 60G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 60G 110G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 110G 160G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 160G 210G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 210G 260G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 260G 310G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 310G 360G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 360G 433G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 433G 506G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 506G 579G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 579G 652G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 652G 725G
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">parted -s /dev/nvme2n1 mkpart primary 725G 800G
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="6-list-disk-by-id">6. list disk by-id&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># list sas disk by id&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ls -al &lt;span class="p">|&lt;/span> grep wwn
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">9&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 wwn-0x5000c500943614ab -&amp;gt; ../../sdl
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">9&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 wwn-0x5000cca0801eb5c8 -&amp;gt; ../../sdc
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">9&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 wwn-0x5000cca080248ff8 -&amp;gt; ../../sdh
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">9&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 wwn-0x5000cca08024aca8 -&amp;gt; ../../sdk
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">9&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 wwn-0x5000cca08024f0d0 -&amp;gt; ../../sdi
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">9&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 wwn-0x5000cca080253734 -&amp;gt; ../../sdj
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">9&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 wwn-0x5000cca0802601cc -&amp;gt; ../../sdf
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">9&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 wwn-0x5000cca0802735ec -&amp;gt; ../../sda
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">9&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 wwn-0x5000cca0802747c8 -&amp;gt; ../../sdd
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">9&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 wwn-0x5000cca0802afad0 -&amp;gt; ../../sdb
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">9&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 wwn-0x5000cca0802ba6ac -&amp;gt; ../../sdg
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">9&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 wwn-0x5000cca080326bd0 -&amp;gt; ../../sde
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#list nvme disk by uuid&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ls -al &lt;span class="p">|&lt;/span> grep nvme-MT
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">13&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 nvme-MT0800KEXUU_CVFT73520058800MGN -&amp;gt; ../../nvme1n1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:07 nvme-MT0800KEXUU_CVFT73520058800MGN-part1 -&amp;gt; ../../nvme1n1p1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:08 nvme-MT0800KEXUU_CVFT73520058800MGN-part10 -&amp;gt; ../../nvme1n1p10
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:08 nvme-MT0800KEXUU_CVFT73520058800MGN-part11 -&amp;gt; ../../nvme1n1p11
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:08 nvme-MT0800KEXUU_CVFT73520058800MGN-part12 -&amp;gt; ../../nvme1n1p12
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 nvme-MT0800KEXUU_CVFT73520058800MGN-part13 -&amp;gt; ../../nvme1n1p13
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 nvme-MT0800KEXUU_CVFT73520058800MGN-part14 -&amp;gt; ../../nvme1n1p14
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 nvme-MT0800KEXUU_CVFT73520058800MGN-part15 -&amp;gt; ../../nvme1n1p15
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 nvme-MT0800KEXUU_CVFT73520058800MGN-part16 -&amp;gt; ../../nvme1n1p16
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 nvme-MT0800KEXUU_CVFT73520058800MGN-part17 -&amp;gt; ../../nvme1n1p17
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 nvme-MT0800KEXUU_CVFT73520058800MGN-part18 -&amp;gt; ../../nvme1n1p18
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:07 nvme-MT0800KEXUU_CVFT73520058800MGN-part2 -&amp;gt; ../../nvme1n1p2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:08 nvme-MT0800KEXUU_CVFT73520058800MGN-part3 -&amp;gt; ../../nvme1n1p3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:08 nvme-MT0800KEXUU_CVFT73520058800MGN-part4 -&amp;gt; ../../nvme1n1p4
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:08 nvme-MT0800KEXUU_CVFT73520058800MGN-part5 -&amp;gt; ../../nvme1n1p5
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:08 nvme-MT0800KEXUU_CVFT73520058800MGN-part6 -&amp;gt; ../../nvme1n1p6
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:07 nvme-MT0800KEXUU_CVFT73520058800MGN-part7 -&amp;gt; ../../nvme1n1p7
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:07 nvme-MT0800KEXUU_CVFT73520058800MGN-part8 -&amp;gt; ../../nvme1n1p8
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:08 nvme-MT0800KEXUU_CVFT73520058800MGN-part9 -&amp;gt; ../../nvme1n1p9
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">13&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 nvme-MT0800KEXUU_CVFT7352005S800MGN -&amp;gt; ../../nvme2n1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:08 nvme-MT0800KEXUU_CVFT7352005S800MGN-part1 -&amp;gt; ../../nvme2n1p1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:09 nvme-MT0800KEXUU_CVFT7352005S800MGN-part10 -&amp;gt; ../../nvme2n1p10
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:09 nvme-MT0800KEXUU_CVFT7352005S800MGN-part11 -&amp;gt; ../../nvme2n1p11
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:10 nvme-MT0800KEXUU_CVFT7352005S800MGN-part12 -&amp;gt; ../../nvme2n1p12
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 nvme-MT0800KEXUU_CVFT7352005S800MGN-part13 -&amp;gt; ../../nvme2n1p13
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 nvme-MT0800KEXUU_CVFT7352005S800MGN-part14 -&amp;gt; ../../nvme2n1p14
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 nvme-MT0800KEXUU_CVFT7352005S800MGN-part15 -&amp;gt; ../../nvme2n1p15
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 nvme-MT0800KEXUU_CVFT7352005S800MGN-part16 -&amp;gt; ../../nvme2n1p16
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 nvme-MT0800KEXUU_CVFT7352005S800MGN-part17 -&amp;gt; ../../nvme2n1p17
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">16&lt;/span> Jul &lt;span class="m">1&lt;/span> 05:49 nvme-MT0800KEXUU_CVFT7352005S800MGN-part18 -&amp;gt; ../../nvme2n1p18
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:09 nvme-MT0800KEXUU_CVFT7352005S800MGN-part2 -&amp;gt; ../../nvme2n1p2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:09 nvme-MT0800KEXUU_CVFT7352005S800MGN-part3 -&amp;gt; ../../nvme2n1p3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:09 nvme-MT0800KEXUU_CVFT7352005S800MGN-part4 -&amp;gt; ../../nvme2n1p4
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:09 nvme-MT0800KEXUU_CVFT7352005S800MGN-part5 -&amp;gt; ../../nvme2n1p5
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:10 nvme-MT0800KEXUU_CVFT7352005S800MGN-part6 -&amp;gt; ../../nvme2n1p6
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:08 nvme-MT0800KEXUU_CVFT7352005S800MGN-part7 -&amp;gt; ../../nvme2n1p7
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:09 nvme-MT0800KEXUU_CVFT7352005S800MGN-part8 -&amp;gt; ../../nvme2n1p8
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lrwxrwxrwx &lt;span class="m">1&lt;/span> root root &lt;span class="m">15&lt;/span> Jul &lt;span class="m">1&lt;/span> 06:09 nvme-MT0800KEXUU_CVFT7352005S800MGN-part9 -&amp;gt; ../../nvme2n1p9
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>after sort the data ,will will got this list&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#sas disk&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/wwn-0x5000cca0802735ec
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/wwn-0x5000cca0802afad0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/wwn-0x5000cca0801eb5c8
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/wwn-0x5000cca0802747c8
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/wwn-0x5000cca080326bd0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/wwn-0x5000cca0802601cc
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/wwn-0x5000cca0802ba6ac
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/wwn-0x5000cca080248ff8
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/wwn-0x5000cca08024f0d0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/wwn-0x5000cca080253734
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/wwn-0x5000cca08024aca8
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/wwn-0x5000c500943614ab
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># nvme&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part4
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part5
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part6
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part7
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part8
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part9
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part10
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part11
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part12
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part4
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part5
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part6
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part7
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part8
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part9
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part10
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part11
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part12
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="7-make-bcache-disk">7. Make Bcache disk&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">make-bcache -C /dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part13 -B /dev/disk/by-id/wwn-0x5000cca0802735ec --writeback
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">make-bcache -C /dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part14 -B /dev/disk/by-id/wwn-0x5000cca0802afad0 --writeback
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">make-bcache -C /dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part15 -B /dev/disk/by-id/wwn-0x5000cca0801eb5c8 --writeback
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">make-bcache -C /dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part16 -B /dev/disk/by-id/wwn-0x5000cca0802747c8 --writeback
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">make-bcache -C /dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part17 -B /dev/disk/by-id/wwn-0x5000cca080326bd0 --writeback
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">make-bcache -C /dev/disk/by-id/nvme-MT0800KEXUU_CVFT73520058800MGN-part18 -B /dev/disk/by-id/wwn-0x5000cca0802601cc --writeback
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">make-bcache -C /dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part13 -B /dev/disk/by-id/wwn-0x5000cca0802ba6ac --writeback
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">make-bcache -C /dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part14 -B /dev/disk/by-id/wwn-0x5000cca080248ff8 --writeback
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">make-bcache -C /dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part15 -B /dev/disk/by-id/wwn-0x5000cca08024f0d0 --writeback
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">make-bcache -C /dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part16 -B /dev/disk/by-id/wwn-0x5000cca080253734 --writeback
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">make-bcache -C /dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part17 -B /dev/disk/by-id/wwn-0x5000cca08024aca8 --writeback
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">make-bcache -C /dev/disk/by-id/nvme-MT0800KEXUU_CVFT7352005S800MGN-part18 -B /dev/disk/by-id/wwn-0x5000c500943614ab --writeback
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="8-reboot-to-make-bcache-works">8. Reboot to make bcache works&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">reboot now
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="9-create-osd">9. create OSD&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">ceph-volume lvm create --bluestore --data /dev/bcache0 --block.wal /dev/nvme1n1p1 --block.db /dev/nvme1n1p7
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ceph-volume lvm create --bluestore --data /dev/bcache1 --block.wal /dev/nvme1n1p2 --block.db /dev/nvme1n1p8
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ceph-volume lvm create --bluestore --data /dev/bcache2 --block.wal /dev/nvme1n1p3 --block.db /dev/nvme1n1p9
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ceph-volume lvm create --bluestore --data /dev/bcache3 --block.wal /dev/nvme1n1p4 --block.db /dev/nvme1n1p10
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ceph-volume lvm create --bluestore --data /dev/bcache4 --block.wal /dev/nvme1n1p5 --block.db /dev/nvme1n1p11
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ceph-volume lvm create --bluestore --data /dev/bcache5 --block.wal /dev/nvme1n1p6 --block.db /dev/nvme1n1p12
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ceph-volume lvm create --bluestore --data /dev/bcache6 --block.wal /dev/nvme2n1p1 --block.db /dev/nvme2n1p7
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ceph-volume lvm create --bluestore --data /dev/bcache7 --block.wal /dev/nvme2n1p2 --block.db /dev/nvme2n1p8
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ceph-volume lvm create --bluestore --data /dev/bcache8 --block.wal /dev/nvme2n1p3 --block.db /dev/nvme2n1p9
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ceph-volume lvm create --bluestore --data /dev/bcache9 --block.wal /dev/nvme2n1p4 --block.db /dev/nvme2n1p10
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ceph-volume lvm create --bluestore --data /dev/bcache10 --block.wal /dev/nvme2n1p5 --block.db /dev/nvme2n1p11
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ceph-volume lvm create --bluestore --data /dev/bcache11 --block.wal /dev/nvme2n1p6 --block.db /dev/nvme2n1p12
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Bcache</title><link>https://echowings.github.io/p/bcache/</link><pubDate>Tue, 09 Mar 2021 16:20:42 +0800</pubDate><guid>https://echowings.github.io/p/bcache/</guid><description>&lt;h1 id="how-to-operate-becache">How to operate becache&lt;/h1>
&lt;p>Recently I just try to add bcache with zfs and ceph. So I just wirte down it.&lt;/p>
&lt;h2 id="install-bcache-tools">Install bcache-tools&lt;/h2>
&lt;p>Since bcache implement in Linux kernel. So it will default enabled. We just need to install &lt;code>bcache-tools&lt;/code> to operate bcache.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">apt install -y bcache-tools
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="configure-bcache">Configure bcache&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># make a cache disk for bcache .It must be a ssd disk.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">make-bcache -C /dev/nvme1n1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># make a disk as backend disk.It must be a slow sata/sas disk.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">make-bcache -B /dev/sda
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#if you have a lot of disks like me.your can add backend disk as this.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">make-bcache -B /dev/sd&lt;span class="o">{&lt;/span>a..f&lt;span class="o">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>###Check bcache status&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">lsblk
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="add-bcache-manually">Add bcache manually&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#show `cset.uuid&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">root@xa-autotest-hci01:~# bcache-super-show /dev/sda
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sb.magic ok
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sb.first_sector &lt;span class="m">8&lt;/span> &lt;span class="o">[&lt;/span>match&lt;span class="o">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sb.csum ED8FD98E20A2E91E &lt;span class="o">[&lt;/span>match&lt;span class="o">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sb.version &lt;span class="m">1&lt;/span> &lt;span class="o">[&lt;/span>backing device&lt;span class="o">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">dev.label &lt;span class="o">(&lt;/span>empty&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">dev.uuid e7b350fe-3496-47e2-a276-5faa5637f8a5
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">dev.sectors_per_block &lt;span class="m">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">dev.sectors_per_bucket &lt;span class="m">1024&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">dev.data.first_sector &lt;span class="m">16&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">dev.data.cache_mode &lt;span class="m">1&lt;/span> &lt;span class="o">[&lt;/span>writeback&lt;span class="o">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">dev.data.cache_state &lt;span class="m">1&lt;/span> &lt;span class="o">[&lt;/span>clean&lt;span class="o">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cset.uuid d3fcd781-a10a-453a-96dd-6eba13abfc5b
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>####Add cache disk&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">echo&lt;/span> &lt;span class="s2">&amp;#34;d3fcd781-a10a-453a-96dd-6eba13abfc5b&amp;#34;&lt;/span> &amp;gt; /sys/block/bcache0/bcache/attach
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>if you have a lot of disk, you can add it with for loop.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> i in range&lt;span class="o">{&lt;/span>0..5&lt;span class="o">}&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="k">do&lt;/span> &lt;span class="sb">`&lt;/span>&lt;span class="nb">echo&lt;/span> &lt;span class="s2">&amp;#34;d3fcd781-a10a-453a-96dd-6eba13abfc5b&amp;#34;&lt;/span> &amp;gt; /sys/block/bcache&lt;span class="k">$(&lt;/span>expr &lt;span class="nv">$i&lt;/span>&lt;span class="k">)&lt;/span>/bcache/attach&lt;span class="sb">`&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="k">done&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h4 id="delete-cache-disk">Delete cache disk&lt;/h4>
&lt;p>if you want to delete bcache disk&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">echo&lt;/span> &lt;span class="s2">&amp;#34;d3fcd781-a10a-453a-96dd-6eba13abfc5b&amp;#34;&lt;/span> &amp;gt; /sys/block/bcache0/bcache/detach
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># delete a lot disk &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> i in range&lt;span class="o">{&lt;/span>0..5&lt;span class="o">}&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="k">do&lt;/span> &lt;span class="sb">`&lt;/span>&lt;span class="nb">echo&lt;/span> &lt;span class="s2">&amp;#34;d3fcd781-a10a-453a-96dd-6eba13abfc5b&amp;#34;&lt;/span> &amp;gt; /sys/block/bcache&lt;span class="k">$(&lt;/span>expr &lt;span class="nv">$i&lt;/span>&lt;span class="k">)&lt;/span>/bcache/detach&lt;span class="sb">`&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="k">done&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h4 id="unrigister-cache-disk">unrigister cache disk&lt;/h4>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">umount /dev/bcach0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">echo&lt;/span> &lt;span class="m">1&lt;/span> &amp;gt; /sys/block/bcache0/bcache/stop
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>If you onle have one nvme disk and you can add them all in one
make-bcache -C /dev/nvme1n1 -B /dev/sd{a..f} &amp;ndash;writeback&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">## Change bcache write policy
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">```bash
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">make-bcache -B/dev/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="manually-resigter-udev-disk">Manually resigter udev disk&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">echo&lt;/span> /dev/sdb &amp;gt; /sys/fs/bcache/register
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="change-bcache-write-mode">Change bcache write mode&lt;/h2>
&lt;p>When you run command like &lt;code>make-bcache -C /dev/nvme1n1 -B /dev/sda&lt;/code> it will be set &lt;code>write through&lt;/code> mode. If you have UPS and you can change it to &lt;code>writeback&lt;/code> mode.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">echo&lt;/span> writeback &amp;gt; /sys/block/bcache0/bcache/cache_mode
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="check-bcache-status">Check bcache status&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">cat /sys/block/bcache0/bcache/state
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">no cache: this means you have not attached a caching device to your backing bcache device&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">clean: this means everything is ok. The cache is clean&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">dirty: this means everything is setup fine and that you have enabled writeback and that the cache is dirty&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">inconsistent: you are in trouble because the backing device is not in sync with the caching device&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bcache-status
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lsblk
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="bcache-optimized">bcache optimized&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#modify cache policy &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat /sys/block/bcache0/bcache/cache_mode
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">echo&lt;/span> writeback &amp;gt; /sys/block/bcache0/bcache/cache_mode
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#Disable ip track&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">echo&lt;/span> &lt;span class="m">0&lt;/span> &amp;gt; /sys/fs/bcache/&lt;span class="nv">$CacheSetUUID&lt;/span>/congested_read_threshold_us
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">echo&lt;/span> &lt;span class="m">0&lt;/span> &amp;gt; /sys/fs/bcache/&lt;span class="nv">$CacheSetUUID&lt;/span>/congested_write_threshold_us
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="disable-bcache">disable bcache&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#check `CACHE_SET_UUID`&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ls /sys/fs/bcache
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#Unregister ssd cache disk&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">echo&lt;/span> &lt;span class="m">1&lt;/span> &amp;gt;/sys/fs/bcache/06e33354-7c21-4c52-990d-1a653617ab20/unregister
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#disable backend disk&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">echo&lt;/span> &lt;span class="m">1&lt;/span> &amp;gt; /sys/block/sdb/bcache/stop
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#write back cache to backend disk&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">echo&lt;/span> &lt;span class="m">0&lt;/span> &amp;gt; /sys/block/bcache0/bcache/writeback_percent
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>##Reference&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.geek-share.com/detail/2719485821.html" target="_blank" rel="noopener"
>A block layer cache (bcache)&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.itread01.com/content/1548985342.html" target="_blank" rel="noopener"
>bcache狀態和配置檔案詳細介紹(翻譯自官網）&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://webcache.googleusercontent.com/search?q=cache:9eiyDtuwt9AJ:https://wiki2.xbits.net:4430/storage:%25E7%25BC%2593%25E5%25AD%2598%25E6%258A%2580%25E6%259C%25AF:bcache:bcache%25E6%2589%258B%25E5%2586%258C&amp;#43;&amp;amp;cd=1&amp;amp;hl=en&amp;amp;ct=clnk&amp;amp;gl=hk" target="_blank" rel="noopener"
>bcache手册&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.mr-mao.cn/archives/linux-bcache-performance-testing.html" target="_blank" rel="noopener"
>Linux下SSD缓存加速之bcache试用&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://billtian.github.io/digoal.blog/2016/09/19/01.html" target="_blank" rel="noopener"
>bcache / 如何使用bcache构建LVM,软RAID / 如何优化bcache&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Build Hyper-Converged Infrastructure With Ceph on PVE</title><link>https://echowings.github.io/p/build-hyper-converged-infrastructure-with-ceph-on-pve/</link><pubDate>Mon, 29 Apr 2019 10:56:16 +0800</pubDate><guid>https://echowings.github.io/p/build-hyper-converged-infrastructure-with-ceph-on-pve/</guid><description>&lt;h1 id="build-hyper-converged-infrastructure-with-ceph-on-pve">Build Hyper-Converged Infrastructure with ceph on PVE&amp;quot;&lt;/h1>
&lt;p>Ceph is really a king of SDS(Software Defin Storage). Opensource. After yeasr evolved, It is enterprise-ready. Today I will build a HCI with ceph on proxmox VE 5.x.&lt;/p>
&lt;p>It will end nightmare that someone or montoring system tell you that your service was down, you need be a fire fighter to fixed as soon as possible under tremendous pressure form your boss. With proxmox ve , We can say &amp;rsquo;no more!&amp;quot;&lt;/p>
&lt;p>If one server was down. and HA will handle this issue. HA will take all of virtual machines that hosted on the server which was down, and boot them other nodes automatically.&lt;/p>
&lt;h2 id="perequest">Perequest&lt;/h2>
&lt;h3 id="hardware">Hardware&lt;/h3>
&lt;p>We need got 3 host each of them have two disk(1 for install proxmox-ve 5.x, another for install ceph). Each host i prepared 2 disk . 128GB SSD for intall proxmox ve . another disk is a 1TB sata(It is really slowly, but for lab testing is enough).&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Host name&lt;/th>
&lt;th>Host IP&lt;/th>
&lt;th>Disk01 for install PVE&lt;/th>
&lt;th>Disk02 for ceph&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>pve-test01&lt;/td>
&lt;td>192.168.0.85&lt;/td>
&lt;td>/dev/sda&lt;/td>
&lt;td>/dev/sdb&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>pve-test02&lt;/td>
&lt;td>192.168.0.86&lt;/td>
&lt;td>/dev/sda&lt;/td>
&lt;td>/dev/sdb&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>pve-test03&lt;/td>
&lt;td>192.168.0.87&lt;/td>
&lt;td>/dev/sda&lt;/td>
&lt;td>/dev/sdb&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="software">Software&lt;/h3>
&lt;h4 id="install-proxmox-ve-or-install-proxmox-ve-5x-on-debian-9">Install proxmox ve or install proxmox ve 5.x on debian 9&lt;/h4>
&lt;ul>
&lt;li>Install proxmox VE 5.x, please follow user guide : &lt;a class="link" href="https://blog.stevedong.com/post/proxmox-ve-v5.x-installation/" target="_blank" rel="noopener"
>Proxmoxve VE V5.x Installation&lt;/a>.&lt;/li>
&lt;li>Install Proxmox VE 5.x on debian 9: &lt;a class="link" href="https://blog.stevedong.com/post/proxmox-ve-how-to-install-proxmoxve5-on-debian9/" target="_blank" rel="noopener"
>How to install ProxmoxVE 5.x on Debian 9&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h4 id="delete-disk-partition-on-2rd-disk">Delete disk partition on 2rd disk.&lt;/h4>
&lt;p>If the disk has data, we can use &lt;code>fdisk /dev/sdb&lt;/code> to delete partition of sdb.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">fdisk /dev/sdb
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Repeat enter m to delete partiton of sdb.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Save configuration with w.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">w
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="create-pve-cluster">Create PVE Cluster&lt;/h3>
&lt;h4 id="create-pve-clusteroperating-on-proxmox-ve-node-pve-test01">Create pve cluster:Operating on proxmox ve node &lt;code>pve-test01&lt;/code>&lt;/h4>
&lt;p>Login via ssh to &lt;code>pve-test01&lt;/code> the first proxmox ve node. Use a unique name for your cluster, the name cannot be changed later.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">pvecm create mycloud
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>To check the state of cluster:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">pvecm status
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h4 id="heading">&lt;/h4>
&lt;h4 id="to-join-node-to-pve-cluseroperate-on-proxmox-ve-node">To join node to pve cluser:Operate on proxmox ve node.&lt;/h4>
&lt;p>Run coomand like these on the rest of pve nodes.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 192.168.0.85 is the ip address of pve node `pve-test01`&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pvecm add 192.168.0.85
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#Check pve cluster status on `pve-test02`&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pvecm status
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="setup-ntp-servic-for-proxmox-ve">Setup NTP Servic for proxmox ve&lt;/h3>
&lt;ul>
&lt;li>If you have a lan ntp server setup like this. mutilpy ntp server separated with blank space.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">cat &amp;gt;&amp;gt; /etc/systemd/timesyncd.conf &lt;span class="s">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">NTP=192.168.0.2 192.168.0.3
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>Or if you can sync ntp with aliyun ntp server in china.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">cat &amp;gt;&amp;gt; /etc/systemd/timesyncd.conf &lt;span class="s">&amp;lt;&amp;lt; EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">NTP=ntp1.aliyun.com ntp2.aliyun.com
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Restart systemd-timesyncd service to apply settings&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">systemctl restart systemd-timesyncd
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="ceph-setup">Ceph Setup&lt;/h3>
&lt;h4 id="install-ceph-on-all-of-pve-nodes">Install ceph on all of pve nodes.&lt;/h4>
&lt;p>Run Each host with this command to install ceph on&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#Install ceph&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pveceph install
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h4 id="create-ceph-storage-network-on-master-node">Create ceph storage network on master node&lt;/h4>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># pve-node-01(10.32.0.85)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pveceph init --network 192.168.0.0/24
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h4 id="run-all-the-command-on-each-node">Run all the command on each node&lt;/h4>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create mon moniting service&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pveceph createmon
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create OSD service &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pveceph createdosd /dev/sdb
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Check ost status&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ceph osd stat
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ceph osd tree
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h4 id="create-ceph-cluster-resource-pool-on-master-node">Create ceph cluster resource pool on master node&lt;/h4>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">ceph osd pool creae pvepool1 &lt;span class="m">128&lt;/span> &lt;span class="m">128&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h4 id="copy-storage-id-and-key-to-the-right-position">copy storage ID and key to the right position&lt;/h4>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#Run the command on master node.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">cd&lt;/span> /etc/pve/priv/
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">mkdir ceph
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cp /etc/ceph/ceph.client.admin.keyring ceph/my-ceph-storage.keyring
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h4 id="check-ceph-cluster-status">Check ceph cluster status&lt;/h4>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">ceph -s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h4 id="create-osd-on-proxmox-ve-portal">Create OSD on proxmox ve portal&lt;/h4>
&lt;ol>
&lt;li>your can access portal on each node&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>&lt;a class="link" href="https://192.168.0.85:8006" target="_blank" rel="noopener"
>https://192.168.0.85:8006&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://192.168.0.86:8006" target="_blank" rel="noopener"
>https://192.168.0.86:8006&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://192.168.0.87:8006" target="_blank" rel="noopener"
>https://192.168.0.87:8006&lt;/a>&lt;/li>
&lt;/ul>
&lt;ol start="2">
&lt;li>Create OSD.&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>&amp;ldquo;Datacenter&amp;rdquo; | &amp;ldquo;Storage&amp;rdquo; | &amp;ldquo;Add&amp;rdquo; | &amp;ldquo;RBD&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;h2 id="setup-ha">Setup HA&lt;/h2>
&lt;h3 id="create-vm-or-lxc-on-one-node">Create vm or lxc on one node&lt;/h3>
&lt;p>Create a VM or LXC on one node and run it.&lt;/p>
&lt;h3 id="setup-the-vm-or-lxc-in-ha-mode">Setup the vm or lxc in HA mode&lt;/h3>
&lt;ul>
&lt;li>&amp;ldquo;Datacenter&amp;rdquo; | &amp;ldquo;HA&amp;rdquo; | &amp;ldquo;Resources&amp;rdquo; | &amp;ldquo;Add&amp;rdquo;.&lt;/li>
&lt;li>Add: Resource:Container/Virtual Machine&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>VM:&lt;/td>
&lt;td>${VM-you-created}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max Restart:&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max Relocate:&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Group&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>request State:&lt;/td>
&lt;td>started&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ol>
&lt;li>&lt;a class="link" href="https://pve.proxmox.com/wiki/High_Availability_Cluster" target="_blank" rel="noopener"
>High Availability Cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://pve.proxmox.com/wiki/Proxmox_VE_2.0_Cluster" target="_blank" rel="noopener"
>Proxmox VE 2.0 Cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.kclouder.cn/proxmoxceph/" target="_blank" rel="noopener"
>PROXMOX+CEPH TO BUILD HCI&lt;/a>&lt;/li>
&lt;/ol></description></item></channel></rss>